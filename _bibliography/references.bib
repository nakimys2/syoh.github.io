
@Article{Stompor2001,
  author    = {Radek Stompor and Amedeo Balbi and Julian D. Borrill and Pedro G. Ferreira and Shaul Hanany and Andrew H. Jaffe and Adrian T. Lee and Sang Oh and Bahman Rabii and Paul L. Richards and George F. Smoot and Celeste D. Winant and Jiun-Huei Proty Wu},
  title     = {Making maps of the cosmic microwave background: The {MAXIMA} example},
  journal   = {Physical Review D},
  year      = {2001},
  volume    = {65},
  number    = {2},
  month     = {dec},
  doi       = {10.1103/physrevd.65.022003},
  file      = {:Stompor2001 - Making maps of the cosmic microwave background_ The MAXIMA example.pdf:PDF},
  groups    = {Homepage, MyResearch},
  publisher = {American Physical Society ({APS})},
}

@InProceedings{Racah2016,
  author    = {{Racah}, Evan and {Ko}, Seyoon and {Sadowski}, Peter and {Bhimji}, Wahid and {Tull}, Craig and {Oh}, Sang-Yun and {Baldi}, Pierre and {Prabhat}},
  title     = {{Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks}},
  booktitle = {International Conference on Machine Learning and Applications (ICMLA)},
  year      = {2016},
  abstract  = {Experiments in particle physics produce enormous quantities of data that must be analyzed and interpreted by teams of physicists. This analysis is often exploratory, where scientists are unable to enumerate the possible types of signal prior to performing the experiment. Thus, tools for summarizing, clustering, visualizing and classifying high-dimensional data are essential. In this work, we show that meaningful physical content can be revealed by transforming the raw data into a learned high-level representation using deep neural networks, with measurements taken at the Daya Bay Neutrino Experiment as a case study. We further show how convolutional deep neural networks can provide an effective classification filter with greater than 97% accuracy across different classes of physics events, significantly better than other machine learning approaches.},
  doi       = {10.1109/icmla.2016.0160},
  file      = {:Racah2016 - Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks.pdf:PDF},
  groups    = {TRIPODS, Homepage, MyResearch},
  journal   = {International Conference on Machine Learning and Applications (ICMLA)},
  owner     = {syoh},
  tags      = {coauthor},
  timestamp = {2016.05.01},
}

@InProceedings{Ali2017,
  author    = {Alnur Ali and Kshitij Khare and Sang-Yun Oh and Bala Rajaratnam},
  title     = {{Generalized Pseudolikelihood Methods for Inverse Covariance Estimation}},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  year      = {2017},
  editor    = {Aarti Singh and Jerry Zhu},
  volume    = {54},
  series    = {Proceedings of Machine Learning Research},
  pages     = {280--288},
  address   = {Fort Lauderdale, FL, USA},
  month     = {20--22 Apr},
  publisher = {PMLR},
  abstract  = {We introduce PseudoNet, a new pseudolikelihood-based estimator of the inverse covariance matrix, that has a number of useful statistical and computational properties.  We show, through detailed experiments with synthetic and also real-world finance as well as wind power data, that PseudoNet outperforms related methods in terms of estimation error and support recovery, making it well-suited for use in a downstream application, where obtaining low estimation error can be important.  We also show, under regularity conditions, that PseudoNet is consistent.  Our proof assumes the existence of accurate estimates of the diagonal entries of the underlying inverse covariance matrix; we additionally provide a two-step method to obtain these estimates, even in a high-dimensional setting, going beyond the proofs for related methods.  Unlike other pseudolikelihood-based methods, we also show that PseudoNet does not saturate, i.e., in high dimensions, there is no hard limit on the number of nonzero entries in the PseudoNet estimate.  We present a fast algorithm as well as screening rules that make computing the PseudoNet estimate over a range of tuning parameters tractable.},
  file      = {:Ali2017 - Generalized Pseudolikelihood Methods for Inverse Covariance Estimation.pdf:PDF;:Ali2017 - Generalized Pseudolikelihood Methods for Inverse Covariance Estimation-Supplementary-Material.pdf:PDF},
  groups    = {TRIPODS, Homepage, MyResearch},
  tags      = {coauthor},
  url       = {http://proceedings.mlr.press/v54/ali17a.html},
}

@InProceedings{Koanantakool2016,
  author    = {Penporn Koanantakool and Ariful Azad and Aydin Buluc and Dmitriy Morozov and Sang-Yun Oh and Leonid Oliker and Katherine Yelick},
  title     = {Communication-Avoiding Parallel Sparse-Dense Matrix-Matrix Multiplication},
  booktitle = {2016 {IEEE} International Parallel and Distributed Processing Symposium ({IPDPS})},
  year      = {2016},
  month     = {may},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  abstract  = {Multiplication of a sparse matrix with a dense matrix is a building block of an increasing number of applications in many areas such as machine learning and graph algorithms. However, most previous work on parallel matrix multiplication considered only both dense or both sparse matrix operands. This paper analyzes the communication lower bounds and compares the communication costs of various classic parallel algorithms in the context of sparse-dense matrix-matrix multiplication. We also present new communication-avoiding algorithms based on a 1D decomposition, called 1.5D, which - while suboptimal in dense-dense and sparse-sparse cases - outperform the 2D and 3D variants both theoretically and in practice for sparse-dense multiplication. Our analysis separates one-time costs from per iteration costs in an iterative machine learning context. Experiments demonstrate speedups up to 100x over a baseline 3D SUMMA implementation and show parallel scaling over 10 thousand cores.},
  doi       = {10.1109/ipdps.2016.117},
  file      = {:Koanantakool2016 - Communication-Avoiding Parallel Sparse-Dense Matrix-Matrix Multiplication.pdf:PDF},
  groups    = {TRIPODS, Homepage, MyResearch},
}

@Article{Hanany2000,
  author    = {S. Hanany and P. Ade and A. Balbi and J. Bock and J. Borrill and A. Boscaleri and P. de Bernardis and P. G. Ferreira and V. V. Hristov and A. H. Jaffe and A. E. Lange and A. T. Lee and P. D. Mauskopf and C. B. Netterfield and S. Oh and E. Pascale and B. Rabii and P. L. Richards and G. F. Smoot and R. Stompor and C. D. Winant and J. H. P. Wu},
  title     = {MAXIMA-1: A Measurement of the Cosmic Microwave Background Anisotropy on Angular Scales of 10 arcminutes to 5 degrees},
  journal   = {The Astrophysical Journal},
  year      = {2000},
  volume    = {545},
  number    = {1},
  pages     = {L5--L9},
  month     = {dec},
  doi       = {10.1086/317322},
  file      = {:Hanany2000 - MAXIMA-1_ A Measurement of the Cosmic Microwave Background Anisotropy on Angular Scales of 10 arcminutes to 5 degrees.pdf:PDF},
  groups    = {Homepage, MyResearch},
  publisher = {{IOP} Publishing},
}

@Article{Jaffe2001,
  author    = {A. H. Jaffe and P. A. R. Ade and A. Balbi and J. J Bock and J. R. Bond and J. Borrill and A. Boscaleri and K. Coble and B. P. Crill and P. de Bernardis and P. Farese and P. G. Ferreira and K. Ganga and M. Giacometti and S. Hanany and E. Hivon and V. V. Hristov and A. Iacoangeli and A. E. Lange and A. T. Lee and L. Martinis and S. Masi and P. D. Mauskopf and A. Melchiorri and T. Montroy and C. B. Netterfield and S. Oh and E. Pascale and F. Piacentini and D. Pogosyan and S. Prunet and B. Rabii and S. Rao and P. L. Richards and G. Romeo and J. E. Ruhl and F. Scaramuzzi and D. Sforna and G. F. Smoot and R. Stompor and C. D. Winant and J. H. P. Wu},
  title     = {Cosmology from {MAXIMA}-1, {BOOMERANG}, and {COBE} {DMR} Cosmic Microwave Background Observations},
  journal   = {Physical Review Letters},
  year      = {2001},
  volume    = {86},
  number    = {16},
  pages     = {3475--3479},
  month     = {apr},
  doi       = {10.1103/physrevlett.86.3475},
  file      = {:Jaffe2001 - Cosmology from MAXIMA-1, BOOMERANG, and COBE DMR Cosmic Microwave Background Observations.pdf:PDF},
  groups    = {Homepage, MyResearch},
  publisher = {American Physical Society ({APS})},
}

@InCollection{Oh2014,
  author    = {Oh, Sang-Yun and Dalal, Onkar and Khare, Kshitij and Rajaratnam, Bala},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  title     = {Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {667--675},
  abstract  = {Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of ℓ1 penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing ℓ1-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for ℓ1-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.},
  file      = {:Oh2014 - Optimization Methods for Sparse Pseudo Likelihood Graphical Model Selection.pdf:PDF},
  groups    = {MyResearch},
  software  = {https://bitbucket.org/sangoh/gconcordopt/src/default/},
  url       = {http://papers.nips.cc/paper/5576-optimization-methods-for-sparse-pseudo-likelihood-graphical-model-selection.pdf},
}

@Article{Wu2001,
  author    = {J. H. P. Wu and A. Balbi and J. Borrill and P. G. Ferreira and S. Hanany and A. H. Jaffe and A. T. Lee and S. Oh and B. Rabii and P. L. Richards and G. F. Smoot and R. Stompor and C. D. Winant},
  title     = {Asymmetric Beams in Cosmic Microwave Background Anisotropy Experiments},
  journal   = {The Astrophysical Journal Supplement Series},
  year      = {2001},
  volume    = {132},
  number    = {1},
  pages     = {1--17},
  month     = {jan},
  doi       = {10.1086/318947},
  file      = {:Wu2001 - Asymmetric Beams in Cosmic Microwave Background Anisotropy Experiments.pdf:PDF},
  groups    = {Homepage, MyResearch},
  publisher = {{IOP} Publishing},
}

@Article{Levinson2012,
  author    = {Douglas F. Levinson and Jianxin Shi and Kai Wang and Sang Oh and Brien Riley and Ann E. Pulver and Dieter B. Wildenauer and Claudine Laurent and Bryan J. Mowry and Pablo V. Gejman and Michael J. Owen and Kenneth S. Kendler and Gerald Nestadt and Sibylle G. Schwab and Jacques Mallet and Deborah Nertney and Alan R. Sanders and Nigel M. Williams and Brandon Wormley and Virginia K. Lasseter and Margot Albus and Stephanie Godard-Bauch{\'{e}} and Madeline Alexander and Jubao Duan and Michael C. O'Donovan and Dermot Walsh and Anthony O'Neill and George N. Papadimitriou and Dimitris Dikeos and Wolfgang Maier and Bernard Lerer and Dominique Campion and David Cohen and Maurice Jay and Ayman Fanous and Peter Eichhammer and Jeremy M. Silverman and Nadine Norton and Nancy Zhang and Hakon Hakonarson and Cynthia Gao and Ami Citri and Mark Hansen and Stephan Ripke and Frank Dudbridge and Peter A. Holmans and},
  title     = {Genome-Wide Association Study of Multiplex Schizophrenia Pedigrees},
  journal   = {American Journal of Psychiatry},
  year      = {2012},
  volume    = {169},
  number    = {9},
  pages     = {963--973},
  month     = {sep},
  doi       = {10.1176/appi.ajp.2012.11091423},
  file      = {:Levinson2012 - Genome-Wide Association Study of Multiplex Schizophrenia Pedigrees.pdf:PDF},
  groups    = {Homepage, MyResearch},
  publisher = {American Psychiatric Publishing},
}

@Article{Levinson2011,
  author    = {Douglas F. Levinson and Jubao Duan and Sang Oh and Kai Wang and Alan R. Sanders and Jianxin Shi and Nancy Zhang and Bryan J. Mowry and Ann Olincy and Farooq Amin and C. Robert Cloninger and Jeremy M. Silverman and Nancy G. Buccola and William F. Byerley and Donald W. Black and Kenneth S. Kendler and Robert Freedman and Frank Dudbridge and Itsik Pe{\textquotesingle}er and Hakon Hakonarson and Sarah E. Bergen and Ayman H. Fanous and Peter A. Holmans and Pablo V. Gejman},
  title     = {Copy Number Variants in Schizophrenia: Confirmation of Five Previous Findings and New Evidence for 3q29 Microdeletions and {VIPR}2 Duplications},
  journal   = {American Journal of Psychiatry},
  year      = {2011},
  volume    = {168},
  number    = {3},
  pages     = {302--316},
  month     = {mar},
  doi       = {10.1176/appi.ajp.2010.10060876},
  file      = {:Levinson2011 - Copy Number Variants in Schizophrenia_ Confirmation of Five Previous Findings and New Evidence for 3q29 Microdeletions and VIPR2 Duplications.pdf:PDF},
  groups    = {Homepage, MyResearch},
  publisher = {American Psychiatric Publishing},
}

@InProceedings{Koanantakool2018,
  author    = {Penporn Koanantakool and Alnur Ali and Ariful Azad and Aydin Buluc and Dmitriy Morozov and Leonid Oliker and Katherine Yelick and Sang-Yun Oh},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  title     = {Communication-Avoiding Optimization Methods for Distributed Massive-Scale Sparse Inverse Covariance Estimation},
  year      = {2018},
  address   = {Playa Blanca, Lanzarote, Canary Islands},
  editor    = {Amos Storkey and Fernando Perez-Cruz},
  month     = {09--11 Apr},
  pages     = {1376--1386},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {84},
  abstract  = {Across a variety of scientific disciplines, sparse inverse covariance estimation is a popular tool for capturing the underlying dependency relationships in multivariate data. Unfortunately, most estimators are not scalable enough to handle the sizes of modern high-dimensional data sets (often on the order of terabytes), and assume Gaussian samples. To address these deficiencies, we introduce HP-CONCORD, a highly scalable optimization method for estimating a sparse inverse covariance matrix based on a regularized pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal gradient method uses a novel communication-avoiding linear algebra algorithm and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving parallel scalability on problems with up to ≈819 billion parameters (1.28 million dimensions); even on a single node, HP-CONCORD demonstrates scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to estimate the underlying dependency structure of the brain from fMRI data, and use the result to identify functional regions automatically. The results show good agreement with a clustering from the neuroscience literature.},
  file      = {:Koanantakool2018 - Communication Avoiding Optimization Methods for Distributed Massive Scale Sparse Inverse Covariance Estimation.pdf:PDF},
  groups    = {Homepage, MyResearch},
  software  = {http://kaewgb.com/hp-concord/},
  url       = {http://proceedings.mlr.press/v84/koanantakool18a.html},
}

@InProceedings{Bhimji2017,
  author    = {Wahid Bhimji and Evan Racah and Seyoon Ko and Peter Sadowski and Craig Tull and Sang-Yun Oh and Prabhat},
  booktitle = {Proceedings of 38th International Conference on High Energy Physics {\textemdash} {PoS}({ICHEP}2016)},
  title     = {Exploring Raw {HEP} Data using Deep Neural Networks at {NERSC}},
  year      = {2017},
  month     = {feb},
  publisher = {Sissa Medialab},
  abstract  = {High Energy Physics has made use of artificial neural networks for some time. Recently, however, there has been considerable development outside the HEP community, particularly in deep neural networks for the purposes of image recognition. We describe the deep-learning infrastructure at NERSC, and analyses built on top of this. These are capable of revealing meaningful physical content by transforming the raw data from particle physics experiments into learned high-level representations using deep convolutional neural networks (CNNs), including in unsupervised modes where no input physics knowledge or training data is used. 
Here we describe in detail a project for the Daya Bay Neutrino Experiment showing both unsupervised learning and how supervised convolutional deep neural networks can provide an effective classification filter with significantly better accuracy than other machine learning methods. These approaches have significant applications for use in other experiments triggers, data quality monitoring or physics analyses.},
  doi       = {10.22323/1.282.0889},
  file      = {:Bhimji2017 - Exploring Raw HEP Data Using Deep Neural Networks at NERSC.pdf:PDF},
  groups    = {MyResearch},
}

@Article{Khare2014,
  author    = {Kshitij Khare and Sang-Yun Oh and Bala Rajaratnam},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title     = {A convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence guarantees},
  year      = {2014},
  month     = {sep},
  number    = {4},
  pages     = {803--825},
  volume    = {77},
  abstract  = {Sparse high dimensional graphical model selection is a topic of much interest in modern day statistics. A popular approach is to apply l1‐penalties to either parametric likelihoods, or regularized regression/pseudolikelihoods, with the latter having the distinct advantage that they do not explicitly assume Gaussianity. As none of the popular methods proposed for solving pseudolikelihood‐based objective functions have provable convergence guarantees, it is not clear whether corresponding estimators exist or are even computable, or if they actually yield correct partial correlation graphs. We propose a new pseudolikelihood‐based graphical model selection method that aims to overcome some of the shortcomings of current methods, but at the same time retain all their respective strengths. In particular, we introduce a novel framework that leads to a convex formulation of the partial covariance regression graph problem, resulting in an objective function comprised of quadratic forms. The objective is then optimized via a co‐ordinatewise approach. The specific functional form of the objective function facilitates rigorous convergence analysis leading to convergence guarantees; an important property that cannot be established by using standard results, when the dimension is larger than the sample size, as is often the case in high dimensional applications. These convergence guarantees ensure that estimators are well defined under very general conditions and are always computable. In addition, the approach yields estimators that have good large sample properties and also respect symmetry. Furthermore, application to simulated and real data, timing comparisons and numerical convergence is demonstrated. We also present a novel unifying framework that places all graphical pseudolikelihood methods as special cases of a more general formulation, leading to important insights.},
  doi       = {10.1111/rssb.12088},
  file      = {:Khare2014 - A Convex Pseudolikelihood Framework for High Dimensional Partial Correlation Estimation with Convergence Guarantees.pdf:PDF},
  groups    = {MyResearch},
  publisher = {Wiley},
  software  = {https://cran.r-project.org/web/packages/gconcord},
}

@Article{Zapata2019,
  author      = {Javier Zapata and Sang-Yun Oh and Alexander Petersen},
  journal     = {Submitted},
  title       = {Partial Separability and Functional Graphical Models for Multivariate Gaussian Processes},
  year        = {2019},
  abstract    = {The covariance structure of multivariate functional data can be highly complex, especially if the multivariate dimension is large, making extension of statistical methods for standard multivariate data to the functional data setting quite challenging. For example, Gaussian graphical models have recently been extended to the setting of multivariate functional data by applying multivariate methods to the coefficients of truncated basis expansions. However, a key difficulty compared to multivariate data is that the covariance operator is compact, and thus not invertible. The methodology in this paper addresses the general problem of covariance modeling for multivariate functional data, and functional Gaussian graphical models in particular. As a first step, a new notion of separability for multivariate functional data is proposed, termed partial separability, leading to a novel Karhunen-Lo\`eve-type expansion for such data. Next, the partial separability structure is shown to be particularly useful in order to provide a well-defined Gaussian graphical model that can be identified with a sequence of finite-dimensional graphical models, each of fixed dimension. This motivates a simple and efficient estimation procedure through application of the joint graphical lasso. Empirical performance of the method for graphical model estimation is assessed through simulation and analysis of functional brain connectivity during a motor task.},
  date        = {2019-10-07},
  eprint      = {1910.03134v2},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Zapata2019 - Partial Separability and Functional Graphical Models for Multivariate Gaussian Processes.pdf:PDF},
  groups      = {MyResearch},
  keywords    = {stat.ME, math.ST, stat.ML, stat.TH},
  software    = {https://github.com/javzapata/fgm},
  url         = {https://arxiv.org/abs/1910.03134},
}

@InProceedings{CisnerosVelarde2020,
  image     = {},
  author    = {Cisneros-Velarde, Pedro and Petersen, Alexander and Oh, Sang-Yun},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  title     = {Distributionally Robust Formulation and Model Selection for the Graphical Lasso},
  year      = {2020},
  address   = {Online},
  editor    = {Chiappa, Silvia and Calandra, Roberto},
  month     = {26--28 Aug},
  pages     = {756--765},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {108},
  abstract  = {Building on a recent framework for distributionally robust optimization, we consider inverse covariance matrix estimation for multivariate data. A novel notion of Wasserstein ambiguity set is provided that is specifically tailored to this problem, leading to a tractable class of regularized estimators. Penalized likelihood estimators for Gaussian data, specifically the graphical lasso estimator, are special cases. Consequently, a direction connection is made between the radius of the Wasserstein ambiguity and the regularization parameter, so that the level of robustness of the estimator is shown to correspond to the level of confidence with which the ambiguity set contains a distribution with the population covariance. A unique feature of the formulation is that the radius can be expressed in closed-form as a function of the ordinary sample covariance matrix. Taking advantage of this finding, a simple algorithm is developed to determine a regularization parameter for graphical lasso, using only the bootstrapped sample covariance matrices, rendering computationally expensive repeated evaluation of the graphical lasso algorithm unnecessary. Alternatively, the distributionally robust formulation can also quantify the robustness of the corresponding estimator if one uses an off-the-shelf method such as cross-validation. Finally, a numerical study is performed to analyze the robustness of the proposed method relative to other automated tuning procedures used in practice.},
  file      = {:CisnerosVelarde2020 - Distributionally Robust Formulation and Model Selection for the Graphical Lasso.pdf:PDF;:CisnerosVelarde2020 - Distributionally Robust Formulation and Model Selection for the Graphical Lasso - Supplementary Material.pdf:PDF},
  groups    = {MyResearch},
  software  = {https://github.com/dddlab/robust-selection},
  url       = {http://proceedings.mlr.press/v108/cisneros20a.html},
}

@Article{Khare2019,
  author    = {Kshitij Khare and Sang-Yun Oh and Syed Rahman and Bala Rajaratnam},
  journal   = {Machine Learning},
  title     = {A scalable sparse Cholesky based approach for learning high-dimensional covariance matrices in ordered data},
  year      = {2019},
  month     = {jun},
  number    = {12},
  pages     = {2061--2086},
  volume    = {108},
  doi       = {10.1007/s10994-019-05810-5},
  file      = {:Khare2019 - A Scalable Sparse Cholesky Based Approach for Learning High Dimensional Covariance Matrices in Ordered Data.pdf:PDF;:Khare2019 - A Scalable Sparse Cholesky Based Approach for Learning High Dimensional Covariance Matrices in Ordered Data - Supplementary Material.pdf:PDF},
  groups    = {MyResearch},
  publisher = {Springer Science and Business Media {LLC}},
}
